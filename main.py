import os
import uuid
import logging
import requests
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ConversationHandler, CallbackContext
import base64
import time
from runwayml import RunwayML
from openai import OpenAI
from moviepy import VideoFileClip, concatenate_videoclips, AudioFileClip

# API –∫–ª—é—á–∏
TELEGRAM_API_KEY = 'fill'
OPENAI_API_KEY = "fill"
SPEECH_FOLDER_ID = "fill"
SPEECH_API_KEY = "fill"
RUNWAY_API_TOKEN = 'fill'


PRODUCT_NAME, PRODUCT_FEATURES, TARGET_AUDIENCE, DURATION, TONE, NOTES, IMAGE, CHOICE_VOICE, CUSTOM_VOICE_TEXT = range(9)

logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                    level=logging.INFO)
logger = logging.getLogger(__name__)


def get_script_from_api(product_name, features, target_audience, duration, tone, notes):

    prompt = f"""
    Create a script for a YouTube Shorts advertisement lasting {duration} seconds for the product: {product_name}.
    Features: {features}
    Target Audience: {target_audience}
    Tone: {tone}
    Special Requests: {notes}
    Script format:
    The product MUST be shown in EVERY scene.
    Scene descriptions must be long and profound.
    Every movement in each scene MUST be described - there MUST NOT be broad descriptions.
    Scenes must feature different locations and different characters (no character should appear in more than one scene).
    Break the video into scenes of approximately 5 seconds each.
    DO NOT make a scene compilation of other scenes.
    For each scene, provide it in the following specific format:
    Title: Scene x
    What happens: A detailed description of what happens in the scene. Make sure to describe the specific action and product usage.
    Camera: Camera angle, camera movements (if any), and any important details about framing.
    Visual elements: Setting, colors, textures, lighting, and how the product should be visually integrated into the scene.
    Sound: Background track, sound effects.
    Notes: Any additional brief explanations or instructions if necessary.
    Product: How the product should be presented in the shot (close-ups, on the table, in use, etc.).
    Make sure the script is:
    Written in clear English.
    The product is shown in EVERY scene.
    Scenes do not contain the same locations and characters (different characters in each scene).
    Creative and dynamic (suitable for the Shorts format).
    Understandable even without sound, since many people watch YouTube Shorts without audio.
    Suitable for editing with text overlays and quick transitions.
    No repeating locations or characters.
    The shots of the product must be creative and have cool visual hooks.
        """
    client = OpenAI(api_key=OPENAI_API_KEY)
    response = client.responses.create(
        model="gpt-4.1",
        input=prompt
    )
    return response.output_text


def generate_image_from_scene(scene_description, image_path):
    prompt = """
    I will give you the description of a scene.
    You should generate a photorealistic image of the start of this scene as if it was shot on Sony ZV-E10 Kit 16-50 using in the picture the exact product from the reference image
    the result must be 9x16. 
    Make sure not to create any morphing, excessive body parts 
    The product from the reference image must be EXACT same one.
    The vibe must be aesthetic and cinematic highly detailed
    Scene:
    """
    client = OpenAI(api_key=OPENAI_API_KEY)
    result = client.images.edit(
        model="gpt-image-1",
        image=[
            open(image_path, "rb"),
        ],
        prompt=prompt+scene_description
    )
    image_base64 = result.data[0].b64_json

    return image_base64


def download_video(video_url, save_path):
    response = requests.get(video_url)
    if response.status_code == 200:
        print('trying to download')
        with open(save_path, "wb") as f:
            print('writing')
            f.write(response.content)
    else:
        print(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {video_url}")


def create_video_with_runway(images, scenes):
    client = RunwayML(api_key=RUNWAY_API_TOKEN)
    videos = []
    for i in range(len(images)):
        prompt = scenes[i]
        if len(scenes[i]) > 1000:
            prompt = prompt[:999]
        task = client.image_to_video.create(
            model='gen4_turbo',
            prompt_image=f"data:image/png;base64,{images[i]}",
            duration=5,
            prompt_text=prompt,
            ratio="720:1280"
        )
        task_id = task.id
        print("Task created, polling...")
        time.sleep(10)
        task = client.tasks.retrieve(task_id)
        while task.status not in ['SUCCEEDED', 'FAILED']:
            print(f"Status: {task.status}... Waiting...")
            task = client.tasks.retrieve(task_id)
        print(task)
        print(task.output)
        if task.status == 'SUCCEEDED':
            video_url = task.output[0]
            videos.append(video_url)
            print('–í–∏–¥–µ–æ –≥–æ—Ç–æ–≤–æ! –°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ:', video_url)

    os.makedirs('videos', exist_ok=True)
    video_paths = []
    for idx, url in enumerate(videos):
        unique_filename = str(uuid.uuid4())
        save_path = f"videos/video_{unique_filename}.mp4"
        download_video(url, save_path)
        video_paths.append(save_path)
    clips = [VideoFileClip(path) for path in video_paths]
    final_clip = concatenate_videoclips(clips, method="compose")
    unique_filename = str(uuid.uuid4())
    output_path = f"videos/final_{unique_filename}.mp4"
    final_clip.write_videofile(output_path, codec="libx264", audio_codec="aac")
    final_clip.close()
    return output_path


async def start(update: Update, context: CallbackContext):
    await update.message.reply_text(
        "–ü—Ä–∏–≤–µ—Ç! –Ø –ø–æ–º–æ–≥—É —Å–æ–∑–¥–∞—Ç—å —Ä–µ–∫–ª–∞–º–Ω—ã–π —Ä–æ–ª–∏–∫ –¥–ª—è –≤–∞—à–µ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞. –ù–∞—á–Ω—ë–º?\n–°–Ω–∞—á–∞–ª–∞ –º–Ω–µ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –Ω–µ–º–Ω–æ–≥–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
        " –ù–∞–ø–∏—à–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ –∏ –±—Ä–µ–Ω–¥–∞."
    )
    return PRODUCT_NAME


async def product_name(update: Update, context: CallbackContext):
    context.user_data['product_name'] = update.message.text
    await update.message.reply_text(
        "–û—Ç–ª–∏—á–Ω–æ! –û–ø–∏—à–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–æ–≤–∞—Ä–∞, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—á–µ—Ç—Å—è –ø–æ–¥—Å–≤–µ—Ç–∏—Ç—å –≤ –≤–∏–¥–µ–æ.")
    return PRODUCT_FEATURES


async def product_features(update: Update, context: CallbackContext):
    context.user_data['product_features'] = update.message.text
    await update.message.reply_text("–ö–∞–∫–∞—è —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è –¥–ª—è —ç—Ç–æ–≥–æ —Ç–æ–≤–∞—Ä–∞?")
    return TARGET_AUDIENCE


async def target_audience(update: Update, context: CallbackContext):
    context.user_data['target_audience'] = update.message.text
    await update.message.reply_text(
        "–û–ø–∏—à–∏—Ç–µ –æ–±—â–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–æ–ª–∏–∫–∞, –∫–æ—Ç–æ—Ä–æ–µ –≤—ã —Ö–æ—Ç–∏—Ç–µ —Å–æ–∑–¥–∞—Ç—å. (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–µ–≥–∫–∏–π –º–æ—Ç–∏–≤–∏—Ä—É—é—â–∏–π —Ä–æ–ª–∏–∫ –∏–ª–∏ —Å–µ—Ä—å–µ–∑–Ω–∞—è —Ä–µ–∫–ª–∞–º–∞)")
    return DURATION


async def duration(update: Update, context: CallbackContext):
    context.user_data['duration'] = update.message.text
    await update.message.reply_text("–°–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥ –¥–æ–ª–∂–µ–Ω –¥–ª–∏—Ç—å—Å—è —Ä–æ–ª–∏–∫? (–æ—Ç 10 –¥–æ 60)")
    return TONE


async def tone(update: Update, context: CallbackContext):
    context.user_data['tone'] = update.message.text
    await update.message.reply_text("–ï—Å—Ç—å –ª–∏ –∫–∞–∫–∏–µ-—Ç–æ –æ—Å–æ–±—ã–µ –ø–æ–∂–µ–ª–∞–Ω–∏—è –¥–ª—è —Å—Ü–µ–Ω–∞—Ä–∏—è?")
    return NOTES


async def notes(update: Update, context: CallbackContext):
    context.user_data['notes'] = update.message.text
    await update.message.reply_text("–¢–µ–ø–µ—Ä—å –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —á–µ—Ç–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞.")
    return IMAGE


def generate_voice(voice_over):
    url = "https://tts.api.cloud.yandex.net/speech/v1/tts:synthesize"

    headers = {
        "Authorization": f"Bearer {SPEECH_API_KEY}"
    }

    data = {
        "text": voice_over,
        "lang": "ru-RU",
        "voice": "ermil",
        "emotion": "good",
        "speed": 1.2,
        "folderId": SPEECH_FOLDER_ID,
        "format": "mp3"
    }
    os.makedirs('voice', exist_ok=True)
    unique_filename = str(uuid.uuid4()) + ".mp3"
    voice_path = f"voice/{unique_filename}"
    response = requests.post(url, headers=headers, data=data)
    print(response)
    if response.status_code == 200:
        with open(voice_path, "wb") as f:
            f.write(response.content)

    return voice_path


async def generate_shorts(update: Update, context: CallbackContext):
    script = get_script_from_api(
        context.user_data['product_name'],
        context.user_data['product_features'],
        context.user_data['target_audience'],
        context.user_data['duration'],
        context.user_data['tone'],
        context.user_data['notes']
    )
    scenes = script.split("Scene ")

    images = []
    for i, scene in enumerate(scenes[1:], 1):
        scene_description = f"Scene {i}: {scene}"
        output_image = generate_image_from_scene(scene_description, context.user_data['image_path'])
        images.append(output_image)

    output_path = create_video_with_runway(images, scenes)
    context.user_data['video_path'] = output_path

    prompt = f"–°–æ–∑–¥–∞–π –æ–∑–≤—É—á–∫—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –¥–ª—è —Ä–µ–∫–ª–∞–º–Ω–æ–≥–æ —Ä–æ–ª–∏–∫–∞ —Å–æ —Å–ª–µ–¥—É—é—â–∏–º —Å—Ü–µ–Ω–∞—Ä–∏–µ–º: {script}. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É—á—Ç–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω—É –∑–∞–ø–∏—Å–∏ {context.user_data['duration']}"
    client = OpenAI(api_key=OPENAI_API_KEY)
    response = client.responses.create(
        model="gpt-4.1",
        input=prompt
    )
    voice_over = response.output_text
    context.user_data['voice_over'] = voice_over

    with open(output_path, "rb") as video_file:
        await update.message.reply_video(video=video_file)

    await update.message.reply_text(
        f"–í–æ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –æ–∑–≤—É—á–∫–∏:\n\n{voice_over}\n\n"
        "–í—ã–±–µ—Ä–∏—Ç–µ, –∫–∞–∫ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å:\n"
        "1. ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n"
        "2. üìù –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–≤–æ–π —Ç–µ–∫—Å—Ç –æ–∑–≤—É—á–∫–∏\n"
        "3. üîá –û—Å—Ç–∞–≤–∏—Ç—å –≤–∏–¥–µ–æ –±–µ–∑ –æ–∑–≤—É—á–∫–∏",
    )
    return CHOICE_VOICE


async def voice_choice(update: Update, context: CallbackContext):
    user_text = update.message.text.strip().lower()
    if "1" in user_text or "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å" in user_text:
        voice_path = generate_voice(context.user_data['voice_over'])
        return await finalize_video_with_voice(update, context, voice_path)

    elif "2" in user_text or "—Å–≤–æ–π" in user_text:
        await update.message.reply_text("–•–æ—Ä–æ—à–æ! –û—Ç–ø—Ä–∞–≤—å—Ç–µ —Å–≤–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–∑–≤—É—á–∫–∏.")
        return CUSTOM_VOICE_TEXT

    elif "3" in user_text or "–±–µ–∑" in user_text:
        return ConversationHandler.END
    else:
        await update.message.reply_text("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ 1, 2 –∏–ª–∏ 3.")
        return CHOICE_VOICE


async def custom_voice_text(update: Update, context: CallbackContext):
    user_text = update.message.text
    voice_path = generate_voice(user_text)
    return await finalize_video_with_voice(update, context, voice_path)


async def finalize_video_with_voice(update: Update, context: CallbackContext, voice_path):
    video = VideoFileClip(context.user_data['video_path'])

    if voice_path:
        audio = AudioFileClip(voice_path)
        video.audio = audio

    unique_filename = str(uuid.uuid4()) + ".mp4"
    final_output_path = f"videos/final_{unique_filename}"
    video.write_videofile(final_output_path, codec="libx264", audio_codec="aac")

    with open(final_output_path, "rb") as final_video:
        await update.message.reply_video(video=final_video)

    return ConversationHandler.END


async def image(update: Update, context: CallbackContext):
    os.makedirs('product_images', exist_ok=True)

    unique_filename = str(uuid.uuid4()) + ".jpg"

    file = await update.message.photo[-1].get_file()
    image_path = f"product_images/{unique_filename}"

    await file.download_to_drive(image_path)

    context.user_data['image_path'] = image_path

    return await generate_shorts(update, context)


async def cancel(update: Update, context: CallbackContext):
    await update.message.reply_text("–ü—Ä–æ—Ü–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ –æ—Ç–º–µ–Ω–µ–Ω.")
    return ConversationHandler.END


def main():
    application = Application.builder().token(TELEGRAM_API_KEY).build()

    conv_handler = ConversationHandler(
        entry_points=[CommandHandler('start', start)],
        states={
            PRODUCT_NAME: [MessageHandler(filters.TEXT & ~filters.COMMAND, product_name)],
            PRODUCT_FEATURES: [MessageHandler(filters.TEXT & ~filters.COMMAND, product_features)],
            TARGET_AUDIENCE: [MessageHandler(filters.TEXT & ~filters.COMMAND, target_audience)],
            DURATION: [MessageHandler(filters.TEXT & ~filters.COMMAND, duration)],
            TONE: [MessageHandler(filters.TEXT & ~filters.COMMAND, tone)],
            NOTES: [MessageHandler(filters.TEXT & ~filters.COMMAND, notes)],
            IMAGE: [MessageHandler(filters.PHOTO, image)],
            CHOICE_VOICE: [MessageHandler(filters.TEXT & ~filters.COMMAND, voice_choice)],
            CUSTOM_VOICE_TEXT: [MessageHandler(filters.TEXT & ~filters.COMMAND, custom_voice_text)]
        },
        fallbacks=[CommandHandler('cancel', cancel)],
    )

    application.add_handler(conv_handler)

    application.run_polling()


if __name__ == '__main__':
    main()
